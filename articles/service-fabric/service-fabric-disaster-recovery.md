---
title: Azure Service Fabric でのディザスター リカバリー
description: Azure Service Fabric では、あらゆる種類の障害に対処するために必要な機能が提供されています。 この記事では、発生する可能性がある災害の種類とそれらに対処する方法について説明します。
author: masnider
ms.topic: conceptual
ms.date: 08/18/2017
ms.author: masnider
ms.openlocfilehash: f9bde0f81dc364aaa09dc9763f2014d83f992371
ms.sourcegitcommit: f915d8b43a3cefe532062ca7d7dbbf569d2583d8
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 03/05/2020
ms.locfileid: "78298294"
---
# <a name="disaster-recovery-in-azure-service-fabric"></a>Azure Service Fabric でのディザスター リカバリー
高可用性を実現するうえで欠かせないのは、サービスがあらゆる種類の障害を切り抜けられるようにすることです。 これは、計画外の障害や、制御できない障害に関しては特に重要です。 この記事では、正しくモデル化および管理されていない場合に、災害につながる可能性がある一般的な障害モードをいくつか取り上げて説明します。 さらに、軽減策や、災害が発生した場合に実行するアクションについても解説します。 その目的は、計画的または計画外の障害が発生したときに、ダウンタイムやデータ損失のリスクを軽減または排除することです。

## <a name="avoiding-disaster"></a>災害の回避
Service Fabric の主な目的は、一般的な障害が災害につながらないように、環境とサービスの両方をモデル化できるよう支援することです。 

一般的に、災害/障害のシナリオには 2 つの種類があります。

1. ハードウェアまたはソフトウェアのエラー
2. 操作上のエラー

### <a name="hardware-and-software-faults"></a>ハードウェアおよびソフトウェアのエラー
ハードウェアとソフトウェアのエラーは予測できません。 障害を最も簡単に切り抜けるには、ハードウェアまたはソフトウェアのエラーの境界で、より多くのサービス コピーを実行します。 たとえば、サービスが 1 つの特定のマシンでのみ実行されている場合、そのマシンで障害が発生するということは、サービスで災害が発生するということです。 この災害は、サービスを確実に複数のマシンで実行することによって、簡単に回避できます。 1 台のマシンの障害によって実行中のサービスが中断しないように、テストを行う必要もあります。 容量計画によって置換インスタンスを他の場所に作成すれば、容量が減っても、残りのサービスが過負荷になりません。 このパターンは、回避しようとしている障害の種類にかかわらず有効です。 たとえば次のようになります。 SAN の障害が心配であれば、複数の SAN で実行します。 サーバー ラックの損失が心配であれば、複数のラックで実行します。 データセンターの損失が心配であれば、複数の Azure リージョンまたはデータセンターでサービスを実行します。 

このタイプのスパン モードで実行しても、同時に発生する一部の障害の影響はまだ受けます。しかし、1 つの障害と、種類によっては複数の障害 (1 つの VM またはネットワーク リンクの障害など) については自動的に処理されます (もはや "災害" ではありません)。 Service Fabric には、クラスターを拡張するメカニズムが多数用意されており、障害が発生したノードとサービスを元に戻します。 また、このような計画外の障害が本当の意味での災害にならないように、Service Fabric では、サービスのインスタンスを多数実行できます。

障害に対応できる規模でのデプロイを実現できないのには理由があるのでしょう。 たとえば、ハードウェア リソースのコストが、障害が発生する可能性を考えたときに、惜しまずに支払える金額を超えているのかもしれません。 分散アプリケーションを扱う場合は、地理的に離れた場所における追加の通信ホップまたは状態レプリケーション コストにより、許容できない待機時間が発生することがあります。 この線引きはアプリケーションごとに異なります。 特にソフトウェア エラーについては、スケールしようとしているサービスでエラーが発生している可能性があります。 この場合、コピーを増やしても災害を防ぐことはできません。障害の状態が、すべてのインスタンス間で相互に関連付けられているためです。

### <a name="operational-faults"></a>操作上のエラー
サービスが世界規模で分散され、あらゆる冗長性が実現されていても、災害を引き起こすイベントは発生します。 たとえば、ユーザーは誤ってサービスの DNS 名を再構成したり、完全に削除したりするものです。 ステートフルな Service Fabric サービスがあり、誰かがそのサービスを誤って削除したとします。 軽減策がなければ、サービスとそのサービスの状態はすべて失われます。 こうした種類の操作上の災害 ("不測") には、通常の計画外の障害とは異なる復旧対策と手順が必要です。 

このような操作上のエラーを回避する最善の方法を次に示します
1. 環境への作業のためのアクセスを制限する
2. 危険な操作を厳密に監査する
3. 自動化を実施し、手動または帯域外の変更を防止するほか、実際の環境に対する特定の変更を適用前に検証する
4. 破壊的な操作が "ソフト" であることを確認する。 ソフト操作はすぐには有効になりません。一定の時間内であれば操作を元に戻すことができます

Service Fabric には、クラスター操作に対する[ロールベース](service-fabric-cluster-security-roles.md)のアクセス制御など、操作上のエラーを回避するためのメカニズムがいくつか用意されています。 ただし、このような操作上のエラーのほとんどでは、組織的な取り組みと他のシステムが必要です。 Service Fabric には、操作上のエラーを切り抜けるためのメカニズムがいくつか用意されています。中でもよく知られているのは、ステートフル サービスのバックアップと復元です。

## <a name="managing-failures"></a>障害の管理
Service Fabric では、障害を常に自動管理することを目標としています。 ただし、一部の障害については、処理のための追加コードがサービスに必要です。 また、安全性とビジネス継続性の理由により、自動的に対処するべき "_ではない_" 種類の障害もあります。 

### <a name="handling-single-failures"></a>1 つの障害への対処
1 台のマシンで障害が発生する理由はさまざまです。 電源、ネットワーク ハードウェアの障害など、ハードウェアが原因である場合があります。 また、ソフトウェアが原因であることもあります。 これには実際のオペレーティング システムとサービス自体の障害が含まれます。 Service Fabric では、このような障害、たとえば、ネットワークの問題によりマシンが他のマシンから切り離されている、といった状況が自動的に検出されます。

サービスの種類に関係なく、実行されているインスタンスが 1 つだと、そのコードの 1 つのコピーが何らかの理由で失敗した場合に、そのサービスでダウンタイムが発生します。 

すべての障害に対して最も簡単に対応するには、ご自身のサービスを、既定で、複数のノードで実行するようにします。 ステートレス サービスの場合、これを行うには、`InstanceCount` を 1 よりも大きな値に設定します。 ステートフル サービスの場合、推奨最小値は必ず `TargetReplicaSetSize` と `MinReplicaSetSize` が 3 以上です。 サービス コードのコピーを複数実行すると、サービスによって、すべての障害が確実に自動処理されます。 

### <a name="handling-coordinated-failures"></a>組織的障害への対処
クラスター内での組織的障害の原因は、計画的または計画外インフラストラクチャ障害と変更、または計画的ソフトウェア変更のいずれかです。 Service Fabric は、組織的障害が発生しているインフラストラクチャ ゾーンを、障害ドメインとしてモデル化します。 組織的ソフトウェア変更が発生する領域は、アップグレード ドメインとしてモデル化されます。 障害ドメインおよびアップグレード ドメインの詳細については、[こちらのドキュメント](service-fabric-cluster-resource-manager-cluster-description.md)を参照してください。このドキュメントでは、クラスター トポロジと定義について説明しています。

既定では、Service Fabric は、障害ドメインとアップグレード ドメインを考慮して、サービスを実行する場所を計画します。 また、Service Fabric では、既定で複数の障害ドメインとアップグレード ドメインにわたってサービスを実行しようとするため、計画的または計画外変更が発生しても、サービスは使用可能な状態のままです。 

たとえば、電源の障害により、マシン ラックで同時に障害が発生したとします。 サービスの複数のコピーが実行されているため、障害ドメインでの多数のマシンの喪失は、特定のサービスに対する単一障害の別の一例に過ぎません。 障害ドメインの管理が、サービスの高可用性の確保に欠かせないのはこのためです。 Azure で Service Fabric を実行している場合、障害ドメインは自動的に管理されます。 他の環境では、そうではない可能性があります。 オンプレミスで独自のクラスターを作成する場合は、障害ドメインのレイアウトを必ず正しく計画し、マップしてください。

アップグレード ドメインは、ソフトウェア アップグレードを同時に実行する領域をモデル化するときに役に立ちます。 このため、アップグレード ドメインは、多くの場合、計画的アップグレード中にソフトウェアが停止される境界も定義します。 Service Fabric とサービスの両方のアップグレードが同じモデルに従います。 ローリング アップグレード、アップグレード ドメイン、および予期しない変更による影響をクラスターとサービスが受けないようにするうえで役立つ Service Fabric 正常性モデルの詳細については、次のドキュメントを参照してください。

 - [アプリケーションのアップグレード](service-fabric-application-upgrade.md)
 - [アプリケーション アップグレードのチュートリアル](service-fabric-application-upgrade-tutorial.md)
 - [Service Fabric 正常性モデル](service-fabric-health-introduction.md)

[Service Fabric Explorer](service-fabric-visualizing-your-cluster.md) で提供されるクラスター マップを使用して、クラスターのレイアウトを視覚化できます。

<center>

![Service Fabric Explorer での障害ドメインに分散されたノードの表示][sfx-cluster-map]
</center>

> [!NOTE]
> 障害領域のモデル化、ローリング アップグレード、サービス コードと状態の多くのインスタンスの実行、障害ドメインとアップグレード ドメインでサービスを確実に実行するための配置ルール、および組み込みの正常性の監視は、通常の操作上の問題や障害が災害につながるのを未然に防ぐために、Service Fabric に用意されている機能の**一部**にすぎません。 
>

### <a name="handling-simultaneous-hardware-or-software-failures"></a>ハードウェアまたはソフトウェアの同時障害への対処
ここまでは 1 つの障害について説明しました。 おわかりのように、障害ドメインとアップグレード ドメインで実行されているコード (および状態) のコピーを複数保持するだけで、ステートレス サービスとステートフル サービスの両方を簡単に処理できます。 複数の障害がランダムに同時発生することがあります。 こうした障害は、実際の災害につながる可能性が高くなります。


### <a name="random-failures-leading-to-service-failures"></a>サービス障害につながるランダムに発生する障害

#### <a name="stateless-services"></a>ステートレス サービス
サービスの `InstanceCount` は、実行されている必要のあるインスタンスの数を示します。 いずれか (またはすべて) のインスタンスに障害が発生すると、その対応として、Service Fabric は自動的に置換インスタンスを他のノード上に作成します。 Service Fabric は、サービスのインスタンス数が望ましい数に戻るまで、置換インスタンスを作成し続けます。
もう 1 つの例として、ステートレス サービスの `InstanceCount` が -1 である場合、クラスター内の各ノード上で 1 つのインスタンスが実行されている必要があることを意味します。 万一そのいずれかのインスタンスに障害が発生した場合、Service Fabric はサービスが望ましい状態になっていないことを検出し、インスタンスが不足しているノード上にインスタンスを作成しようと試みます。

#### <a name="stateful-services"></a>ステートフル サービス
ステートフル サービスには、次の 2 種類があります。
1. 状態が永続化されるステートフル
2. 状態が永続化されないステートフル (状態はメモリに格納される)

ステートフル サービスの障害復旧は、ステートフル サービスの種類と、サービスに存在するレプリカの数および障害の発生したレプリカの数によって異なります。

クォーラムとは何でしょうか。
ステートフル サービスでは、レプリカ (プライマリとアクティブ セカンダリ) 間で受信データがレプリケートされます。 レプリカの過半数がデータを受信した場合、データはクォーラム コミットされていると見なされます (5 つのレプリカなら 3 が "_クォーラム_")。 つまりどの時点においても、最新のデータを持つレプリカが、クォーラム数以上は存在することが保証されます。 レプリカに障害が発生した場合 (たとえば 5 つのレプリカのうち 2 つに障害が発生した場合)、復旧の可否は、クォーラム値を使用すれば計算できます (5 つのレプリカのうち残り 3 つは依然として稼動状態であるため、少なくとも 1 つのレプリカには完全なデータが存在する)。

次に、クォーラム損失について考えてみましょう。
クォーラム数のレプリカで障害が発生すると、パーティションがクォーラム損失状態として宣言されます。 たとえば、パーティションに 5 つのレプリカがあるとします。つまり、少なくとも 3 つのレプリカに完全なデータがあることが保証されるということです。 クォーラム数 (5 つのうち 3 つ) のレプリカで障害が発生した場合、残りのレプリカ (5 つのうち 2 つ) にパーティションを復元できるだけのデータがあるかどうかは、Service Fabric には判断できません。

ステートフル サービスについて災害が発生したかどうかを判断し、それを管理するプロセスは、3 つの段階に従って実行されます。

1. クォーラムの損失があるかどうかを確認する
    - ステートフル サービスのレプリカの過半数が同時にダウンしているときは、クォーラム損失が宣言されます。
2. クォーラムの損失が永続的かどうかを確認する
   - ほとんどの場合、障害は一時的なものです。 プロセス、ノード、VM が再起動され、ネットワーク パーティションは修復します。 ただし、障害が永続的である場合もあります。 
     - 永続化状態でないサービスの場合、クォーラムまたはレプリカで障害が発生すると、"_直ちに_" 永続的なクォーラム損失が発生します。 Service Fabric は、非永続的なステートフル サービスでクォーラムの損失を検出すると、(潜在的な) データ損失を宣言することによって直ちに手順 3. に処理を進めます。 レプリカが復旧されたとしても、非永続というサービスの性質上、データは既に失われており、Service Fabric にとって、レプリカの復旧を待っても無駄であることは自明であるため、データ損失として処理を進めることは道理にかなっています。
     - 永続的なステートフル サービスの場合、クォーラム数以上のレプリカで障害が発生すると、Service Fabric は、レプリカが復旧してクォーラムが復元するのを待ちます。 これにより、影響を受けるサービス パーティション ("レプリカ セット") に対するすべての "_書き込み_" でサービス停止が発生します。 ただし、一貫性の保証は少なくなりますが、読み取りはまだ可能です。 続行することは (潜在的な) データ損失イベントであり、他のリスクを伴うため、Service Fabric がクォーラムの復元を待つ既定の時間は無制限です。
3. 実際にデータ損失があったかどうかを確認し、バックアップから復元する
   
   Service Fabric が `OnDataLossAsync` メソッドを呼び出す場合、それは常に、データ損失の _疑いがある_ ためです。 Service Fabric により、この呼び出しは確実に "_最適_" な残りのレプリカに配信されます。 これは、最も進捗しているレプリカです。 常に、データ損失の _疑いがある_ という理由は、残りのレプリカが実際に、プライマリが停止したときの状態とまったく同じ状態を持っている可能性があることです。 しかし、比較対象となる状態がなければ、Service Fabric やオペレーターが適切な方法でそれを確信することはできません。 この時点で、Service Fabric は、他のレプリカが戻らないこともわかっています。 これは、クォーラム損失が解決されるのを待つことをやめたとき行われた決定です。 サービスに対する最善なアクションは、通常、アクションを凍結し、特定の管理介入を待つことです。 では、`OnDataLossAsync` メソッドの一般的な実装では何を行うのでしょう。
   1. 最初に、`OnDataLossAsync` がトリガーされたことをログに記録し、必要な管理アラートを起動します。
   1. 通常、この時点で、一時停止し、さらなる決定と、手動アクションが実行されるのを待ちます。 これは、バックアップを使用できる場合でも、準備が必要な可能性があるためです。 たとえば、2 つの異なるサービスで情報を調整する場合、復元が発生したときに、その 2 つのサービスに関連する情報の一貫性を確保するために、こうしたバックアップの変更が必要になることがあります。 
   1. 多くの場合、サービスの他のテレメトリや消費データも存在します。 このメタデータは、他のサービスまたはログに含まれている可能性があります。 この情報を使用して、バックアップに存在しない、またはこの特定のレプリカにレプリケートされなかった呼び出しが、プライマリで受信および処理されたかどうかを判断します。 復元を実現するには、これを再生するか、バックアップに追加しなければならないことがあります。  
   1. 残りのレプリカの状態を、バックアップに含まれるものを比較できます。 Service Fabric の信頼できるコレクションを使用すると、このためのツールやプロセスを入手できます。詳細については、[こちらの記事](service-fabric-reliable-services-backup-restore.md)を参照してください。 その目的は、レプリカ内の状態が十分かどうか、また、バックアップで何が不足しているかを確認することです。
   1. 比較の後、必要に応じて復元が行われ、状態が変更されると、サービス コードは true を返します。 レプリカが使用可能な最善の状態のコピーであると判断された場合、変更は行われず、false を返します。 True は、"_他_" の残りのレプリカが、このレプリカと整合性がとれていない可能性があることを示します。 残りのレプリカは削除され、このレプリカから再作成されます。 False は、状態の変更は行われていないため、他のレプリカをそのまま保持できることを意味します。 

サービスが運用環境にデプロイされる前に、サービス作成者が潜在的なデータ損失と障害のシナリオを実施することが非常に重要です。 データ損失の可能性から保護するには、すべてのステートフル サービスの geo 冗長ストアへの[状態のバックアップ](service-fabric-reliable-services-backup-restore.md)を定期的に行うことが重要です。 また、それを復元する機能を確保する必要もあります。 さまざまなサービスのバックアップがそれぞれ異なるタイミングで行われるため、復元後、こうしたサービスのビューが相互に一貫性があることを確認する必要があります。 たとえば、あるサービスが数値を生成して格納し、その数値を他のサービスに送信したとします。送信先のサービスも、受け取った数値を格納します。 復元後、2 番目のサービスに数値があり、最初のサービスにはないことに気が付きました。これはバックアップに、この操作が含まれていなかったためです。

残りのレプリカがデータ損失のシナリオで続行するには不十分なことがわかり、サービスの状態をテレメトリまたは消費データから再構成できない場合は、バックアップの頻度によって、可能性のある最適な目標復旧時点 (RPO) が決定されます。 Service Fabric には、バックアップからの復元が必要な永続的なクォーラムやデータ損失を含め、さまざまな障害のシナリオをテストするための多数のツールが用意されています。 こうしたシナリオは、Fault Analysis Service によって管理される、Service Fabric の Testability ツールに含まれています。 こうしたツールおよびパターンの詳細については、[こちら](service-fabric-testability-overview.md)を参照してください。 

> [!NOTE]
> システム サービスでもクォーラム損失が発生する可能性があり、その影響は問題のあるサービスに固有です。 たとえば、ネーム サービスでクォーラム損失が発生すると名前の解決に影響があり、フェールオーバー マネージャー サービスの場合は新しいサービスの作成とフェールオーバーがブロックされます。 Service Fabric システム サービスは状態管理のためのサービスと同じパターンに従いますが、それらをクォーラムの損失から潜在的なデータ損失に移動しようとすることはお勧めできません。 代わりに、[サポートを利用](service-fabric-support.md)して、個別の状況に合ったソリューションを判断することをお勧めします。  通常は、ダウンしたレプリカが復旧するまで待つことをお勧めします。
>

#### <a name="troubleshooting-quorum-loss"></a>クォーラム損失のトラブルシューティング

レプリカが、一時的な障害により断続的にダウンすることがあります。 Service Fabric がそれらを稼動状態にしようと試みるので、しばらく待ってください。 レプリカがダウンしたまま所定の時間が経過した場合は、以下のトラブルシューティング手順に従います。
- レプリカがクラッシュしている可能性があります。 レプリカ レベルの正常性レポートおよびアプリケーション ログをチェックしてください。 クラッシュ ダンプを収集し、復元のために必要な措置を講じます。
- レプリカのプロセスが無応答状態になっている可能性があります。 アプリケーション ログを見てそれを確認してください。 プロセス ダンプを収集し、応答しないプロセスを強制終了します。 Service Fabric は、代替プロセスを作成してレプリカの復旧を試みます。
- レプリカをホストしているノードがダウンしている可能性があります。 基になる VM を再起動して、ノードを稼動状態にしてください。

ドライブの故障やマシンの物理的な無応答状態など、レプリカの復旧が不可能な状況もあります。 そのような場合は、レプリカの復旧を待たないよう、Service Fabric に指示する必要があります。
サービスをオンライン状態にするうえでデータ損失のリスクが許容できない場合には、決してこれらの方法は使用しないでください。そのような場合は、物理マシンの復旧に向けてできることをすべて行う必要があります。

以下の手順はデータの損失につながる可能性があります。あらかじめその点を理解したうえで実行してください。
   
> [!NOTE]
> 対象の方法以外で特定のパーティションに対してこれらの方法を使用するのは、"_決して_" 安全ではありません。 
>

- `Repair-ServiceFabricPartition -PartitionId` または `System.Fabric.FabricClient.ClusterManagementClient.RecoverPartitionAsync(Guid partitionId)` API を使用します。 この API では、データ損失のリスクがあるクォーラム損失から復旧するパーティションの ID を指定できます。
- クォーラム損失の状態をサービスに引き起こすような障害がクラスターで頻繁に発生する場合で、なおかつ潜在的な "_データの損失が許容できる_" 場合は、適切な [QuorumLossWaitDuration](https://docs.microsoft.com/powershell/module/servicefabric/update-servicefabricservice?view=azureservicefabricps) 値を指定することで、サービスの自動復旧に役立てることができます。 Service Fabric は、指定された QuorumLossWaitDuration (既定では無期限) 待機した後、復旧を実行します。 この方法は予期しないデータ損失につながるおそれがあるため "_お勧めしません_"。

## <a name="availability-of-the-service-fabric-cluster"></a>Service Fabric クラスターの可用性
一般的には、Service Fabric クラスター自体は高度な分散環境で、単一障害点がありません。 どのノードで障害が発生しても、クラスターの可用性または信頼性で問題が発生することはありません。これは主に Service Fabric システム サービスが、前に説明したものと同じガイドラインに従っているためです。つまり、既定で常に 3 つ以上のレプリカで実行され、こうしたステートレスのシステム サービスはすべてのノードで実行されています。 基になる Service Fabric ネットワークとエラー検出レイヤーは完全に分散されています。 システム サービスは総じてメタデータから再構築できるか、他の場所から状態を再同期する方法を認識しています。 クラスターの可用性が低下する可能性があるのは、これまで説明したようなクォーラム損失状態がシステム サービスで発生した場合です。 このような場合、アップグレードの開始、新しいサービスのデプロイなど、一部の操作をクラスターで実行できないことがありますが、クラスター自体はまだ起動しています。 既に実行中のサービスについては、システム サービスに書き込まなくても継続して動作できるのであれば、こうした状況でも引き続き実行されます。 たとえば、Failover Manager でクォーラム損失が発生しても、すべてのサービスが継続して実行されますが、障害が発生しているサービスについては、Failover Manager の関与が必要であるため、自動的に再起動することはできません。 

### <a name="failures-of-a-datacenter-or-azure-region"></a>データセンターまたはAzure リージョンの障害
まれに、停電やネットワーク切断のために、物理的なデータ センターが一時的に使用できなくなることがあります。 このような場合、そのデータセンターまたは Azure リージョンの Service Fabric クラスターとサービスは使用できなくなります。 ただし、"_データは保存されます_"。 Azure で実行されているクラスターの場合、「[Azure の状態][azure-status-dashboard]」ページで停止に関する更新情報を確認できます。 極めてまれなことですが、データ センターが物理的に一部または全体が破壊された場合、そこでホストされている Service Fabric クラスター、またはその中のサービスが失われる可能性があります。 これには、そのデータセンターやリージョンの外でバックアップされていない状態も含まれます。

1 つのデータセンターやリージョンにおける永続的または持続的な障害を切り抜けるための戦略は 2 つあります。 

1. このような複数のリージョンで Service Fabric クラスターをそれぞれ実行し、こうした環境間でのフェールオーバーとフェールバックのメカニズムをいくつか使用します。 このような複数クラスターのアクティブ/アクティブまたはアクティブ/パッシブ モデルには、追加の管理および操作コードが必要です。 また、あるデータセンターやリージョンで障害が発生したときに、その中のサービスを他のデータセンターやリージョンで使用できるように、サービスのバックアップの調整も必要です。 
2. 複数のデータセンターやリージョンにまたがる 1 つの Service Fabric クラスターを実行します。 これがサポートされる最小構成は 3 つのデータ センターまたはリージョンです。 推奨されるリージョンまたはデータセンターの数は 5 つです。 これには、さらに複雑なクラスター トポロジが必要になります。 ただし、このモデルの利点は、1 つのデータセンターまたはリージョンの障害が、災害から通常の障害に変換される点です。 こうした障害は、1 つのリージョン内のクラスターに対して有効なメカニズムで処理できます。 障害ドメイン、アップグレード ドメイン、および Service Fabric の配置ルールにより、通常の障害が許容されるようにワークロードが分散されます。 この種類のクラスターにおけるサービス操作に役立つポリシーの詳細については、[配置ポリシー](service-fabric-cluster-resource-manager-advanced-placement-rules-placement-policies.md)に関するページをご覧ください。

### <a name="random-failures-leading-to-cluster-failures"></a>クラスター障害につながるランダムに発生する障害
Service Fabric にはシード ノードの概念があります。 これは基になるクラスターの可用性を維持するノードです。 特定の種類のネットワーク障害が発生しているときに、他のノードとのリースを確立し、タイブレーカーとして機能することで、クラスターが確実に稼働し続けるうえで役立ちます。 ランダムに発生する障害によってクラスター内のシード ノードの大部分が削除され、元に戻らない場合、シード ノード クォーラムが失われるとクラスター フェデレーションの輪が崩れ、クラスターに障害が発生します。 Azure では、Service Fabric リソース プロバイダーで Service Fabric クラスターの構成を管理し、既定ではプライマリ ノードの種類のフォールト ドメインとアップグレード ドメインにシード ノードを分散します。プライマリ ノードの種類が Silver または Gold の持続性とマークされている場合、シード ノードを削除するときに、プライマリ ノードの種類をスケールするか手動でシード ノードを削除すると、クラスターではプライマリ ノードの種類の空き領域から別のシードではないノードを昇格しようとします。また、クラスターの信頼性レベルがプライマリ ノードの種類に必要とする領域よりも空き容量が少ない場合は失敗します。

スタンドアロンの Service Fabric クラスターと Azure の両方について、シードを実行するのは "プライマリ ノード タイプ" です。 プライマリ ノード タイプを定義するとき、Service Fabric では、システム サービスごとに最大 9 個のシード ノードと 7 個のレプリカを作成することで、提供されるノード数が自動的に使用されます。 ランダムに発生する障害によって、こうしたシステム サービス レプリカの大部分が同時に削除されると、前に説明したように、システム サービスはクォーラム損失に移行します。 シード ノードの大部分が失われた場合、クラスターは直ちにシャット ダウンします。

## <a name="next-steps"></a>次のステップ
- [Testability フレームワーク](service-fabric-testability-overview.md)
- ディザスター リカバリーと高可用性に関する他のリソースを読みます。 Microsoft は、これらのトピックに関して多数のガイダンスを公開しています。 これらのドキュメントの一部は他の製品で使用するための具体的な方法に関するものですが、 Service Fabric にも適用できる多くの一般的なベスト プラクティスが含まれます。
  - [可用性のチェックリスト](/azure/architecture/checklist/resiliency-per-service)
  - [ディザスター リカバリー訓練の実行](../sql-database/sql-database-disaster-recovery-drills.md)
  - [Azure アプリケーションのディザスター リカバリーと高可用性][dr-ha-guide]
- [Service Fabric のサポート オプション](service-fabric-support.md)について学びます。


<!-- External links -->

[repair-partition-ps]: https://msdn.microsoft.com/library/mt163522.aspx
[azure-status-dashboard]:https://azure.microsoft.com/status/
[azure-regions]: https://azure.microsoft.com/regions/
[dr-ha-guide]: https://msdn.microsoft.com/library/azure/dn251004.aspx


<!-- Images -->

[sfx-cluster-map]: ./media/service-fabric-disaster-recovery/sfx-clustermap.png
