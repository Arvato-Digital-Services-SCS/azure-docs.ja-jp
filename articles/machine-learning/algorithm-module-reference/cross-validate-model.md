---
title: モデルのクロス検証:モジュール リファレンス
titleSuffix: Azure Machine Learning service
description: Azure Machine Learning service でモデルのクロス検証モジュールを使用して、データをパーティション分割することにより、分類モデルまたは回帰モデルのパラメーターの推定値をクロス検証する方法について説明します。
services: machine-learning
ms.service: machine-learning
ms.subservice: core
ms.topic: reference
author: likebupt
ms.author: keli19
ms.date: 10/10/2019
ms.openlocfilehash: a5eea61ee8284010531e80e17bf1110ab470d04c
ms.sourcegitcommit: c22327552d62f88aeaa321189f9b9a631525027c
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 11/04/2019
ms.locfileid: "73510288"
---
# <a name="cross-validate-model"></a>モデルのクロス検証

この記事では、Azure Machine Learning デザイナー (プレビュー) で**モデルのクロス検証**モジュールを使用する方法について説明します。 "*クロス検証*" は、データセットの変動性と、そのデータを使用してトレーニングされたモデルの信頼性の両方を評価するために機械学習でよく使用される重要な手法です。  

**モデルのクロス検証**モジュールは、トレーニングしていない分類モデルまたは回帰モデルと共に、ラベルが付けられたデータセットを入力として受け取ります。 これは、データセットをいくつかのサブセット ("*フォールド*") に分割し、各フォールドでモデルを構築してから、各フォールドの正確性の統計セットを返します。 すべてのフォールドの正確性の統計を比較することにより、データセットの品質を解釈し、モデルがデータの変動の影響を受けやすいかどうかを把握できます。  

クロス検証では、データセットの予測結果と確率も返されるため、予測の信頼性を評価できます。  

### <a name="how-cross-validation-works"></a>クロス検証のしくみ

1. クロス検証では、トレーニング データが複数のパーティション ("*フォールド*" とも呼ばれます) にランダムに分割されます。 

    + データセットをまだパーティション分割していない場合、アルゴリズムの既定値は 10 フォールドになります。 
    + データセットを異なる数のフォールドに分割するには、[パーティションとサンプル](partition-and-sample.md)モジュールを使用して、使用するフォールドの数を指定します。  

2.  このモジュールは、検証に使用するためにフォールド 1 のデータを確保し (これは "*ホールドアウト フォールド*" と呼ばれることもあります)、残りのフォールドを使用してモデルをトレーニングします。 

    たとえば、5 つのフォールドを作成した場合、このモジュールはクロス検証中に 5 つのモデルを生成します。各モデルは、データの 4/5 を使用してトレーニングされ、残りの 1/5 でテストされます。  

3.  各フォールドのモデルのテスト中に、複数の正確性の統計が評価されます。 どの統計が使用されるかは、評価するモデルの種類によって異なります。 分類モデルと回帰モデルの評価には、異なる統計が使用されます。  

4.  すべてのフォールドの構築と評価のプロセスが完了すると、**モデルのクロス検証**によって、すべてのデータについて一連のパフォーマンス メトリックとスコア付け結果が生成されます。 これらのメトリックを調べて、1 つのフォールドに特に高い正確性または低い正確性があるかどうかを確認する必要があります。 

### <a name="advantages-of-cross-validation"></a>クロス検証の利点

モデルを評価するための別の一般的な方法として、[データの分割](split-data.md)を使用してデータをトレーニング セットとテスト セットに分割し、トレーニング データに対してモデルを検証する方法があります。 ただし、クロス検証にはいくつかの利点があります。  

-   クロス検証では、より多くのテスト データが使用されます。

     クロス検証では、より大きなデータ領域内で指定のパラメーターを使用してモデルのパフォーマンスを測定します。 つまり、クロス検証では、トレーニングと評価の両方で、トレーニング データセットの一部ではなく全体が使用されます。 これに対し、ランダム分割から生成されたデータを使用してモデルを検証する場合、通常は、使用可能なデータの 30% 以下に対してのみモデルを評価します。  

     ただし、クロス検証では、大規模なデータセットでモデルを複数回トレーニングして検証するため、ランダム分割での検証よりも計算量が多くなり、はるかに長い時間がかかります。  

-   クロス検証では、モデルだけでなくデータセットも評価されます。

     クロス検証では、モデルの正確性が測定されるだけではなく、データセットがどの程度代表的であるかと、モデルがデータの変動にどの程度影響を受けやすいかについての理解も得られます。  

## <a name="how-to-use-cross-validate-model"></a>モデルのクロス検証を使用する方法

データセットが大きい場合、クロス検証の実行に時間がかかることがあります。  そのため、モデルの構築とテストの初期段階で**モデルのクロス検証**を使用して、モデル パラメーターの妥当性を評価し (計算時間が許容できると仮定)、次に[モデルのトレーニング](train-model.md)モジュールと[モデルの評価](evaluate-model.md)モジュールで確立されたパラメーターを使用してモデルをトレーニングおよび評価できます。

このシナリオでは、**モデルのクロス検証**を使用して、モデルのトレーニングとテストの両方を行います。

1. **モデルのクロス検証**モジュールをパイプラインに追加します。 これは、Azure Machine Learning デザイナーの **[Model Scoring & Evaluation]\(モデルのスコアリングと評価\)** カテゴリにあります。 

2. **分類**または**回帰**モデルの出力を接続します。 

    たとえば、分類に **Two Class Bayes Point Machine** を使用している場合は、必要なパラメーターを使用してモデルを構成し、次に分類子の**未トレーニング モデル**ポートから、**モデルのクロス検証**の対応するポートにコネクタをドラッグします。 

    > [!TIP] 
    > **モデルのクロス検証**によってモデルが評価の一部として自動的にトレーニングされるため、モデルのトレーニングを行う必要はありません。  
3.  **モデルのクロス検証**の**データセット**ポートで、ラベル付けされたトレーニング データセットを接続します。  

4.  **モデルのクロス検証**の **[プロパティ]** ウィンドウで、 **[Launch column selector]\(列セレクターの起動\)** をクリックし、クラス ラベルまたは予測可能な値を含む単一の列を選択します。 

5. 同じデータに対する連続した実行の間でクロス検証の結果を繰り返すことができるようにするには、**ランダム シード** パラメーターの値を設定します。  

6.  パイプラインを実行します。

7. レポートの説明については、「[結果](#results)」セクションを参照してください。

    後で再利用するためにモデルのコピーを取得するには、アルゴリズムを含むモジュールの出力を右クリックし (たとえば、**Two Class Bayes Point Machine**)、 **[Save as Trained Model]\(トレーニング済みのモデルとして保存する\)** をクリックします。

## <a name="results"></a>結果

すべてのイテレーションが完了すると、 **[モデルのクロス検証]** によって、データセット全体のスコアと、モデルの品質の評価に使用できるパフォーマンス メトリックが作成されます。

### <a name="scored-results"></a>スコア付け結果

モジュールの最初の出力では、いくつかの予測値および関連する確率と共に、各行のソース データが提供されます。 

これらの結果を表示するには、パイプラインで、**モデルのクロス検証**モジュールを右 クリックし、 **[Scored results]\(スコア付け結果\)** を選択して、 **[視覚化]** をクリックします。

| 新しい列名      | 説明                              |
| -------------------- | ---------------------------------------- |
| スコア付けラベル        | この列はデータセットの末尾に追加され、各行の予測値が格納されます。 |
| スコア付け確率 | この列はデータセットの末尾に追加され、**スコア付けラベル**の値の推定確率を示します。 |
| フォールド番号          | クロス検証中にデータの各行が割り当てられたフォールドの 0 から始まるインデックスを示します。 |

 ### <a name="evaluation-results"></a>評価結果

2 番目のレポートはフォールドによってグループ化されます。 実行中、**モデルのクロス検証**によって、トレーニング データが *n* フォールド (既定では 10) にランダムに分割されることに留意してください。 データセットでの各イテレーションで、**モデルのクロス検証**は、検証データセットとして 1 つのフォールドを使用し、残りの *n-1* フォールドを使用してモデルをトレーニングします。 各 *n* モデルは、他のすべてのフォールド内のデータに対してテストされます。

このレポートでは、フォールドはインデックス値によって昇順に一覧表示されます。  他の列を並べ替えるには、結果をデータセットとして保存します。

これらの結果を表示するには、パイプラインで、**モデルのクロス検証**モジュールを右 クリックし、 **[Evaluation results by fold]\(フォールドによる評価結果\)** を選択して、 **[視覚化]** をクリックします。


|列名| 説明|
|----|----|
|フォールド番号| 各フォールドの識別子。 5 つのフォールドを作成した場合、5 つのデータのサブセットがあり、0 から 4 の番号が付けられています。
|フォールドの例の数|各フォールドに割り当てられた行の数。 これらはほぼ同じになるはずです。 |


また、評価するモデルの種類に応じて、フォールドごとに次のメトリックが含まれます。 

+ **分類モデル**:精度、再現率、F スコア、AUC、正確性  

+ **回帰モデル**:平均絶対誤差、二乗平均平方根誤差、相対絶対誤差、相対二乗誤差、および決定係数


## <a name="technical-notes"></a>テクニカル ノート  

+ データセットをクロス検証に使用する前に、それを正規化することをお勧めします。 

+ **モデルのクロス検証**では、モデルを複数回トレーニングして検証するため、ランダムに分割されたデータセットを使用してモデルを検証する場合よりも計算量が多く、完了までに時間がかかります。 

+ クロス検証を使用してモデルの正確性を測定する場合、データセットをトレーニング セットとテスト セットに分割する必要はありません。 


## <a name="next-steps"></a>次の手順

Azure Machine Learning service で[使用できる一連のモジュール](module-reference.md)を参照してください。 

