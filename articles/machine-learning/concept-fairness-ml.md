---
title: 機械学習モデルでの公平性を評価して軽減する
titleSuffix: Azure Machine Learning
description: 機械学習モデルでの公平性と、Fairlearn Python パッケージを使用してより公平なモデルを構築する方法について説明します。
services: machine-learning
ms.service: machine-learning
ms.subservice: core
ms.topic: conceptual
ms.author: luquinta
author: luisquintanilla
ms.date: 05/02/2020
ms.openlocfilehash: c21ec0329a7b5716a00262b7422296df3afe208b
ms.sourcegitcommit: bb0afd0df5563cc53f76a642fd8fc709e366568b
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 05/19/2020
ms.locfileid: "83596361"
---
# <a name="fairness-in-machine-learning-models"></a>機械学習モデルでの公平性

機械学習での公平性と、Fairlearn オープンソース Python パッケージを使用してより公平なモデルを構築する方法について説明します。

## <a name="what-is-fairness-in-machine-learning-systems"></a>機械学習システムでの公平性とは

人工知能および機械学習システムでは、不公平な動作が示されることがあります。 不公平な動作を定義する方法の 1 つとして、人への危害や影響があります。 AI システムでは、さまざまな種類の危害が発生する可能性があります。 AI によって発生する 2 つの一般的な種類の危害を次に示します。

- 割り当ての害: AI システムによって、機会、リソース、または情報が増減されます。 たとえば、雇用、入学許可、融資などで、モデルにより、特定のグループの人が、他のグループより、適切な候補をうまく選択される場合があります。

- サービス品質の害: AI システムによる対応のよさが、ユーザーのグループによって異なります。 たとえば、音声認識システムでは、女性に対する対応が男性より悪くなる場合があります。

AI システムの不公平な動作を減らすには、これらの害を評価し、軽減する必要があります。

>[!NOTE]
> 公平性は、社会技術に関する課題です。 正当性や適正手続きなど、公平性の多くの側面は、定量的な公平性のメトリックではキャプチャされません。 また、多くの定量的な公平性のメトリックをすべて同時に満たすことはできません。 目標は、人がさまざまな軽減戦略を評価し、シナリオに適したトレードオフを実現できるようにすることです。

## <a name="fairness-assessment-and-mitigation-with-fairlearn"></a>Fairlearn を使用した公平性の評価と軽減

Fairlearn はオープンソースの Python パッケージであり、機械学習システムの開発者は、それを使用して、システムの公平性を評価し、公平性に関する問題を軽減できます。

Fairlearn には、次の 2 つのコンポーネントがあります。

- 評価ダッシュボード: モデルの予測がさまざまなグループに与える影響を評価するための Jupyter Notebook ウィジェット。 また、公平性とパフォーマンスのメトリックを使用して、複数のモデルを比較することもできます。
- 軽減アルゴリズム: 二項分類と回帰を使用して不公平性を軽減するためのアルゴリズムのセット。

これらのコンポーネントの組み合わせにより、データ サイエンティストやビジネス リーダーは、公平性とパフォーマンスの間のさまざまなトレードオフを確認し、ニーズに最も合った軽減策を選択できます。

## <a name="fairness-assessment"></a>公平性の評価

Fairlearn では、公平性は、次のことを質問する**グループ公平性**と呼ばれるアプローチによって概念化されます: 損害が発生するリスクがあるのはどのグループか。

関連グループ (部分母集団とも呼ばれます) は、**微妙な特徴**または微妙な属性によって定義されます。 微妙な特徴は、ベクトルまたは `sensitive_features` と呼ばれるマトリックスとして Fairlearn の推定器に渡されます。 この用語は、グループの公平性を評価するときに、システム デザイナーがこれらの特徴に敏感でなければならないことを示しています。 注意すべき点は、これらの特徴に個人を特定できる情報によるプライバシーへの影響があるかどうか、ということです。 ただし、"微妙な" という言葉は、予測を行うときにこれらの特徴を使用してはならないという意味ではありません。

評価段階では、公平性は不均衡メトリックによって定量化されます。 **不均衡メトリック**では、比率または相違として、異なるグループの間でのモデルの動作を評価および比較できます。 Fairlearn では、不均衡メトリックの 2 つのクラスがサポートされています。


- モデルのパフォーマンスにおける不均衡: これらのメトリックのセットでは、異なるサブグループ間での、選択されたパフォーマンス メトリックの値の不均衡 (差異) が計算されます。 次に例をいくつか示します。

  - 正解率の不均衡
  - エラー率の不均衡
  - 精度の不均衡
  - リコールの不均衡
  - MAE の不均衡
  - その他多数

- 選択率における不均衡: このメトリックには、異なるサブグループ間での選択率の差が含まれます。 この例としては、ローン承認率の不均衡があります。 選択率とは、各クラスで 1 として分類されるデータポイントの割合 (二項分類)、または予測値の分散 (回帰) を意味します。

## <a name="unfairness-mitigation"></a>不公平性の軽減

### <a name="parity-constraints"></a>不均衡の制約

Fairlearn には、さまざまな不公平性軽減アルゴリズムが含まれています。 これらのアルゴリズムでは、**不均衡の制約**または条件と呼ばれる、予測の動作に対する一連の制約がサポートされています。 不均衡の制約では、予測動作の一部の側面が、微妙な特徴で定義されるグループ (異なる人種など) の間で同等であることが要求されます。 Fairlearn の軽減アルゴリズムでは、このような不均衡の制約を使用して、監視対象の公平性の問題が軽減されます。

Fairlearn では、次の種類の不均衡の制約がサポートされています。

|不均衡の制約  | 目的  |機械学習タスク  |
|---------|---------|---------|
|人口統計の不均衡     |  割り当ての害を軽減する | 二項分類、回帰 |
|均等な確率  | 割り当てとサービス品質の害を診断する | 二項分類        |
|境界グループの損失     |  サービス品質の害を軽減する | 回帰 |

### <a name="mitigation-algorithms"></a>軽減アルゴリズム

Fairlearn では、後処理と削減の不公平性軽減アルゴリズムが提供されています。

- 削減: これらのアルゴリズムでは、標準のブラックボックス ML 推定器 (LightGBM モデルなど) が利用され、一連の再重み付けされたトレーニング データセットを使用して、再トレーニングされたモデルのセットが生成されます。 たとえば、特定の性別の応募者について、重みを加減してモデルが再トレーニングされ、異なる性別グループ間での格差が削減されます。 その後、ユーザーは、精度 (または他のパフォーマンス メトリック) と不均衡の間で最適なトレードオフを提供するモデルを選択できます。これは通常、ビジネス ルールとコストの計算に基づいている必要があります。  
- 後処理: これらのアルゴリズムは、既存の分類子と微妙な特徴を入力として受け取ります。 その後、分類子の予測の変換を導出して、指定された公平性の制約を適用します。 しきい値の最適化の最大の利点は、モデルを再トレーニングする必要がないことによる、シンプルさと柔軟性です。 

| アルゴリズム | 説明 | 機械学習タスク | 微妙な特徴 | サポートされる不均衡の制約 | アルゴリズムの種類 |
| --- | --- | --- | --- | --- | --- |
| `ExponentiatedGradient` | 「[公平な分類のための削減アプローチ](https://arxiv.org/abs/1803.02453)」で説明されている公平な分類のためのブラックボックス アプローチ | 二項分類 | Categorical | [人口統計の不均衡](#parity-constraints)、[均等な確率](#parity-constraints) | 削減 |
| `GridSearch` | 「[公平な分類のための削減アプローチ](https://arxiv.org/abs/1803.02453)」で説明されているブラックボックス アプローチ| 二項分類 | Binary | [人口統計の不均衡](#parity-constraints)、[均等な確率](#parity-constraints) | 削減 |
| `GridSearch` | 境界グループの損失に対するアルゴリズムを使用する公正回帰のグリッド検索バリエーションを実装するブラックボックス アプローチ (「[公正回帰: 定量的な定義と削減に基づくアルゴリズム](https://arxiv.org/abs/1905.12843)」) | 回帰 | Binary | [境界グループの損失](#parity-constraints) | 削減 |
| `ThresholdOptimizer` | ホワイトペーパー「[教師あり学習での機会の均等性](https://arxiv.org/abs/1610.02413)」に基づく後処理アルゴリズム。 この手法では、既存の分類子と微妙な特徴を入力として受け取り、指定された不均衡の制約を適用するために、分類子の予測の単調な変換が導出されます。 | 二項分類 | Categorical | [人口統計の不均衡](#parity-constraints)、[均等な確率](#parity-constraints) | 後処理 |

## <a name="next-steps"></a>次のステップ

- さまざまなコンポーネントの使用方法を学習するには、[Fairlearn GitHub リポジトリ](https://github.com/fairlearn/fairlearn/)と[サンプル ノートブック](https://github.com/fairlearn/fairlearn/tree/master/notebooks)を参照してください。
- [差分プライバシーと WhiteNoise パッケージ](concept-differential-privacy.md)を使用したデータのプライバシーの維持について学習してください。